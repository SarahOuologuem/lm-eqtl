{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43e306-e5f1-4831-bb06-cecdb288fd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stockhaus/miniconda3/envs/sysgen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import pysam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be2f56b-e9fb-4abc-9339-a335bc412afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from encoding_utils import sequence_encoders\n",
    "import helpers.train_eval as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "from models.spec_dss import DSSResNetEmb, SpecAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e119",
   "metadata": {},
   "source": [
    "# Train/Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79b474",
   "metadata": {},
   "source": [
    "**0. Specify input parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edc775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.dataset =  '../datasets/phase3_top10/dataset.parquet'\n",
    "#input_params.species_list = datadir + 'fasta/240_species/240_species.txt'\n",
    "\n",
    "input_params.output_dir = '../test'\n",
    "\n",
    "input_params.split_mask = False\n",
    "input_params.mask_rate = 0.2 #[0.012,0.2]#RAN #single float or 2 floats for reference and alternative\n",
    "input_params.masking = 'none' # stratified_maf or none\n",
    "\n",
    "# input_params.test = False\n",
    "input_params.test = True\n",
    "\n",
    "input_params.get_embeddings = False\n",
    "# input_params.get_embeddings = True\n",
    "input_params.mask_at_test = True\n",
    "\n",
    "input_params.agnostic = True\n",
    "\n",
    "input_params.seq_len = 5000\n",
    "\n",
    "input_params.tot_epochs = 1\n",
    "input_params.fold = 0\n",
    "input_params.Nfolds = 5\n",
    "\n",
    "input_params.train_splits = 1\n",
    "\n",
    "input_params.save_at = [1]\n",
    "input_params.validate_every = 1\n",
    "\n",
    "input_params.d_model = 256\n",
    "input_params.n_layers = 16\n",
    "input_params.dropout = 0.\n",
    "\n",
    "input_params.batch_size = 16\n",
    "input_params.learning_rate = 1e-4\n",
    "input_params.weight_decay = 0\n",
    "\n",
    "if input_params.dataset.endswith('.fa'):\n",
    "    seq_df = pd.read_csv(input_params.dataset + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "elif input_params.dataset.endswith('.parquet'):\n",
    "    seq_df = pd.read_parquet(input_params.dataset).reset_index()\n",
    "    \n",
    "seq_df[['split','sample_id','seg_name']] =  seq_df['seq_name'].str.split(':',expand=True)\n",
    "\n",
    "if not input_params.agnostic:\n",
    "    #for segment-aware model, assign a label to each segment\n",
    "    seg_name = seq_df.seq_name.apply(lambda x:':'.join(x.split(':')[2:]))\n",
    "    segment_encoding = seg_name.drop_duplicates().reset_index(drop=True)\n",
    "    segment_encoding = {seg_name:idx for idx,seg_name in segment_encoding.items()}\n",
    "    seq_df['seg_label'] = seg_name.map(segment_encoding)\n",
    "else:\n",
    "    seq_df['seg_label'] = 0\n",
    "\n",
    "\n",
    "if input_params.test:\n",
    "    seq_df = seq_df[seq_df.split=='test']\n",
    "else:\n",
    "    seq_df = seq_df[seq_df.split!='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d126226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>seq</th>\n",
       "      <th>split</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>seg_name</th>\n",
       "      <th>seg_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20010</th>\n",
       "      <td>test:NA20795:ENSG00000198502.5</td>\n",
       "      <td>FBFFFFFRMRMMMFBFFMBRBRFFRFFFFFFFFMFFBFFFFFFFFB...</td>\n",
       "      <td>test</td>\n",
       "      <td>NA20795</td>\n",
       "      <td>ENSG00000198502.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20011</th>\n",
       "      <td>test:HG00260:ENSG00000214425.1</td>\n",
       "      <td>RRBRRRBRRRBBRRBBBBRBBBRRBRBRRBRRRRBBBRBRRBBRRB...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00260</td>\n",
       "      <td>ENSG00000214425.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>test:HG01632:ENSG00000176681.9</td>\n",
       "      <td>BBRBBBRBBBBBBRBBBBRBBBBBBBBBBBBBBBBBRRBBBBRBBB...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG01632</td>\n",
       "      <td>ENSG00000176681.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20013</th>\n",
       "      <td>test:HG00173:ENSG00000238083.3</td>\n",
       "      <td>RMRRRRRRRRRRRRRRRRFRRRRFRRRRRRMRRRRMRRRRRRRMRR...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00173</td>\n",
       "      <td>ENSG00000238083.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20014</th>\n",
       "      <td>test:HG00178:ENSG00000229450.2</td>\n",
       "      <td>RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRBBRRBRRRRRRRRRRR...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00178</td>\n",
       "      <td>ENSG00000229450.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             seq_name  \\\n",
       "20010  test:NA20795:ENSG00000198502.5   \n",
       "20011  test:HG00260:ENSG00000214425.1   \n",
       "20012  test:HG01632:ENSG00000176681.9   \n",
       "20013  test:HG00173:ENSG00000238083.3   \n",
       "20014  test:HG00178:ENSG00000229450.2   \n",
       "\n",
       "                                                     seq split sample_id  \\\n",
       "20010  FBFFFFFRMRMMMFBFFMBRBRFFRFFFFFFFFMFFBFFFFFFFFB...  test   NA20795   \n",
       "20011  RRBRRRBRRRBBRRBBBBRBBBRRBRBRRBRRRRBBBRBRRBBRRB...  test   HG00260   \n",
       "20012  BBRBBBRBBBBBBRBBBBRBBBBBBBBBBBBBBBBBRRBBBBRBBB...  test   HG01632   \n",
       "20013  RMRRRRRRRRRRRRRRRRFRRRRFRRRRRRMRRRRMRRRRRRRMRR...  test   HG00173   \n",
       "20014  RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRBBRRBRRRRRRRRRRR...  test   HG00178   \n",
       "\n",
       "                seg_name  seg_label  \n",
       "20010  ENSG00000198502.5          0  \n",
       "20011  ENSG00000214425.1          0  \n",
       "20012  ENSG00000176681.9          0  \n",
       "20013  ENSG00000238083.3          0  \n",
       "20014  ENSG00000229450.2          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82414099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14ba6a",
   "metadata": {},
   "source": [
    "**1. Dataset and Dataloader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11c457",
   "metadata": {},
   "source": [
    "Define Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597bf5fb-c432-4eec-93da-17d6f10af4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seq_df, transform, max_augm_shift=0, \n",
    "                 mode='train'):\n",
    "\n",
    "        if input_params.dataset.endswith('.fa'):\n",
    "            self.fasta = pysam.FastaFile(input_params.dataset)\n",
    "        else:\n",
    "            self.fasta = None\n",
    "\n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "        self.max_augm_shift = max_augm_shift\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_df)\n",
    "        # return 2*len(self.seq_df) # times two because returns both haplotypes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.fasta:\n",
    "            seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        else:\n",
    "            seq = self.seq_df.iloc[idx].seq.upper()\n",
    "\n",
    "        shift = np.random.randint(self.max_augm_shift+1) #random shift at training, must be chunk_size-input_params.seq_len\n",
    "\n",
    "        seq = seq[shift:shift+input_params.seq_len] #shift the sequence and limit its size\n",
    "\n",
    "        seg_label = self.seq_df.iloc[idx].seg_label #label for segment-aware training\n",
    "\n",
    "        seq1 = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R') # father sequence\n",
    "        seq2 = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R') # mother sequence\n",
    "\n",
    "        masked_sequence1, target_labels_masked1, target_labels1, _, _ = self.transform(seq1)\n",
    "        masked_sequence2, target_labels_masked2, target_labels2, _, _ = self.transform(seq2)\n",
    "\n",
    "        masked_sequence = torch.vstack((masked_sequence1, masked_sequence2))\n",
    "        seg_label = torch.vstack((torch.tensor(seg_label), torch.tensor(seg_label)))\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "\n",
    "        target_labels_masked = torch.vstack((target_labels_masked1, target_labels_masked2))\n",
    "        target_labels = torch.vstack((target_labels1, target_labels2))\n",
    "        seq = (seq1, seq2)\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "        '''\n",
    "        #for given genotype, randomly choose a haplotype for training/testing\n",
    "        if np.random.rand()>0.5:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R')\n",
    "        else:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf' and not input_params.test:\n",
    "        #    #select mask for the sequence depending on sequence coordinates w.r.t. contig\n",
    "        #    seg_name = self.seq_df.iloc[idx].seg_name\n",
    "        #    seq_mask = meta.loc[seg_name].MASK.values\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq, mask = seq_mask)\n",
    "        #else:\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "\n",
    "        masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "        '''\n",
    "\n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af5510de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data): \n",
    "    #masked sequence\n",
    "    masked_sequence = [x[0][0] for x in data]\n",
    "    masked_sequence = [torch.stack(torch.split(d, 3)) for d in masked_sequence]\n",
    "    masked_sequence = torch.concat(masked_sequence)\n",
    "    #seg labels\n",
    "    seg_labels = [x[0][1] for x in data]\n",
    "    seg_labels = torch.concat(seg_labels).flatten()\n",
    "    # target labels masked\n",
    "    target_labels_masked = [x[1] for x in data]\n",
    "    target_labels_masked = torch.concat(target_labels_masked)\n",
    "    # target labels \n",
    "    target_labels = [x[2] for x in data]\n",
    "    target_labels = torch.concat(target_labels)\n",
    "    #seq\n",
    "    seqs = [x[3] for x in data]\n",
    "    seqs = tuple(chain.from_iterable(seqs))\n",
    "    return (masked_sequence, seg_labels),target_labels_masked, target_labels, seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15637814",
   "metadata": {},
   "source": [
    "Create Dataset and Dataloader for the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ce1c8b-d70c-4fe7-ac01-e86fd30a8dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = None\n",
    "\n",
    "if not input_params.test: #Train and Validate\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate = input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "\n",
    "    #N_train = int(len(seq_df)*(1-input_params.val_fraction))\n",
    "    if input_params.fold is not None:\n",
    "        \n",
    "        samples = seq_df.sample_id.unique()\n",
    "        val_samples = samples[input_params.fold::input_params.Nfolds] \n",
    "        train_df = seq_df[~seq_df.sample_id.isin(val_samples)] \n",
    "        test_df = seq_df[seq_df.sample_id.isin(val_samples)]\n",
    "        test_dataset = SeqDataset(test_df, transform = seq_transform, mode='eval')\n",
    "        test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 0, collate_fn = collate_fn, shuffle = False)\n",
    "    else:\n",
    "        train_df = seq_df\n",
    "        #train_df = seq_df[seq_df.split=='train']\n",
    "        #test_df = seq_df[seq_df.split=='val']\n",
    "  \n",
    "    N_train = len(train_df)\n",
    "    train_fold = np.repeat(list(range(input_params.train_splits)),repeats = N_train // input_params.train_splits + 1 )\n",
    "    train_df['train_fold'] = train_fold[:N_train]\n",
    "    # create training dataset & dataloader \n",
    "    train_dataset = SeqDataset(train_df, transform = seq_transform,  mode='train')\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_fn, shuffle = False)\n",
    "\n",
    "elif input_params.get_embeddings:\n",
    "    if input_params.mask_at_test:\n",
    "        seq_transform = sequence_encoders.RollingMasker(mask_stride = 50, frame = 0)\n",
    "    else:\n",
    "        seq_transform = sequence_encoders.PlainOneHot(frame = 0, padding = 'none')\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = collate_fn, shuffle = False)\n",
    "\n",
    "else: #Test\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate=input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d82217",
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_sequence, species_label), targets_masked, targets, seq = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea18c202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, -100, -100, -100, -100, -100, -100, -100, -100,    0, -100, -100,\n",
       "        -100,    1, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100,\n",
       "        -100,    1, -100,    0,    0, -100, -100,    0, -100,    0, -100, -100,\n",
       "           0, -100, -100,    0, -100, -100,    1, -100, -100,    1, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,\n",
       "           0,    0, -100,    0, -100, -100, -100,    0, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100,    0, -100, -100, -100,    0,    0, -100, -100,\n",
       "        -100, -100,    0, -100, -100, -100,    0, -100, -100, -100, -100, -100,\n",
       "        -100, -100,    0, -100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_masked[0][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0e4aaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sequence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92832778",
   "metadata": {},
   "source": [
    "**2. Define model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad86179c-11e5-4b3b-a389-b8ab1b00bdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seg_encoder = SpecAdd(embed = True, encoder = 'label', Nsegments=seq_df.seg_label.nunique(), d_model = input_params.d_model)\n",
    "\n",
    "model = DSSResNetEmb(d_input = 3, d_output = 3, d_model = input_params.d_model, n_layers = input_params.n_layers, \n",
    "                     dropout = input_params.dropout, embed_before = True, species_encoder = seg_encoder)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = input_params.learning_rate, weight_decay = input_params.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806065f5",
   "metadata": {},
   "source": [
    "**3. Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec227158-c405-4dc7-8f6a-38a83009d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 0\n",
    "\n",
    "if input_params.model_weight:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        #load on gpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight))\n",
    "    else:\n",
    "        #load on cpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight, map_location=torch.device('cpu')))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight, map_location=torch.device('cpu')))\n",
    "\n",
    "    last_epoch = int(input_params.model_weight.split('_')[-3]) #infer previous epoch from input_params.model_weight\n",
    "\n",
    "weights_dir = os.path.join(input_params.output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "\n",
    "if input_params.save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#        milestones=input_params.lr_sch_milestones, gamma=input_params.lr_sch_gamma, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f68efaa-d4a6-424f-a169-6d0d9046ec9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics_to_str(metrics):\n",
    "    loss, accuracy, masked_acc, masked_recall, masked_IQS = metrics\n",
    "    return f'loss: {loss:.4}, acc: {accuracy:.4}, masked acc: {masked_acc:.4}, {misc.print_class_recall(masked_recall, \"masked recall: \")}, masked IQS: {masked_IQS:.4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd086013-fd62-4964-8ccc-8b9d928d727d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Test/Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stockhaus/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################\n",
      "tensor([[[-0.1402, -0.0162,  0.0649,  ..., -0.9979, -0.8577, -0.6262],\n",
      "         [-0.2347, -0.1804, -0.2705,  ..., -0.9223, -0.7644, -0.7041],\n",
      "         [-0.1686, -0.1746, -0.1632,  ..., -0.8667, -0.8799, -0.7828]],\n",
      "\n",
      "        [[-0.1670, -0.0295, -0.1105,  ..., -0.9534, -0.8590, -0.6241],\n",
      "         [-0.1821, -0.1607, -0.2494,  ..., -0.9123, -0.7192, -0.6362],\n",
      "         [-0.2706, -0.2708, -0.2903,  ..., -0.9186, -0.9520, -0.8175]],\n",
      "\n",
      "        [[-0.1349, -0.0015, -0.0850,  ..., -1.0118, -0.8484, -0.7187],\n",
      "         [-0.3303, -0.2986, -0.3216,  ..., -0.8402, -0.6596, -0.6555],\n",
      "         [-0.2664, -0.3419, -0.4059,  ..., -1.0067, -1.0104, -0.8717]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1100,  0.0452,  0.0362,  ..., -0.9691, -0.8833, -0.6307],\n",
      "         [-0.3023, -0.2423, -0.3316,  ..., -0.9073, -0.7076, -0.6565],\n",
      "         [-0.1949, -0.2529, -0.2315,  ..., -0.8254, -0.8944, -0.8001]],\n",
      "\n",
      "        [[-0.0722,  0.0816,  0.0511,  ..., -1.0492, -0.9280, -0.7377],\n",
      "         [-0.1973, -0.2269, -0.3042,  ..., -0.8386, -0.6481, -0.5811],\n",
      "         [-0.2076, -0.2538, -0.2579,  ..., -1.0639, -1.0569, -0.9019]],\n",
      "\n",
      "        [[-0.0922,  0.0607,  0.0190,  ..., -1.0341, -0.9055, -0.7038],\n",
      "         [-0.2080, -0.2447, -0.2980,  ..., -0.8634, -0.6640, -0.6028],\n",
      "         [-0.1783, -0.2510, -0.2461,  ..., -1.0267, -1.0309, -0.8817]]])\n",
      "tensor([[0, 0, 0,  ..., 2, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 2, 1, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n",
      "tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100]])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6316, masked recall: R=0.953;A=0.0161;AVG=0.485, masked acc: 0.6379, masked IQS: 0.005487, loss: 0.9343:   0%|█▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 1/314 [1:11:07<371:01:35, 4267.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m#lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Test/Inference...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     test_metrics, test_embeddings, motif_probas \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - test, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_to_str(test_metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39mget_embeddings:\n",
      "File \u001b[0;32m/mnt/c/Users/stock/Documents/Uni/3.Semester/Systems_genetics/lm-eqtl/model/helpers/train_eval.py:106\u001b[0m, in \u001b[0;36mmodel_eval\u001b[0;34m(model, optimizer, dataloader, device, get_embeddings, temperature, silent)\u001b[0m\n\u001b[1;32m    103\u001b[0m targets_masked \u001b[38;5;241m=\u001b[39m targets_masked\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    104\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 106\u001b[0m logits, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature:\n\u001b[1;32m    109\u001b[0m     logits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m temperature\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/stock/Documents/Uni/3.Semester/Systems_genetics/lm-eqtl/model/models/spec_dss.py:202\u001b[0m, in \u001b[0;36mDSSResNetEmb.forward\u001b[0;34m(self, x, xs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     z \u001b[38;5;241m=\u001b[39m norm(z\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Apply S4 block: we ignore the state input and output\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m z, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Dropout on the output of the S4 block\u001b[39;00m\n\u001b[1;32m    205\u001b[0m z \u001b[38;5;241m=\u001b[39m dropout(z)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/stock/Documents/Uni/3.Semester/Systems_genetics/lm-eqtl/model/models/dss.py:469\u001b[0m, in \u001b[0;36mDSS.forward\u001b[0;34m(self, u, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(y))\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransposed: y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 469\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/c/Users/stock/Documents/Uni/3.Semester/Systems_genetics/lm-eqtl/model/models/dss.py:103\u001b[0m, in \u001b[0;36mTransposedLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m... u l, v u -> ... v l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/opt_einsum/contract.py:507\u001b[0m, in \u001b[0;36mcontract\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen_expression:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ContractExpression(full_str, contraction_list, constants_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meinsum_kwargs)\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontraction_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/opt_einsum/contract.py:573\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43m_tensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/opt_einsum/sharing.py:131\u001b[0m, in \u001b[0;36mtensordot_cache_wrap.<locals>.cached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(tensordot)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_tensordot\u001b[39m(x, y, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# hash based on the (axes_x,axes_y) form of axes\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     _save_tensors(x, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/opt_einsum/contract.py:374\u001b[0m, in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Base tensordot.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m fn \u001b[38;5;241m=\u001b[39m backends\u001b[38;5;241m.\u001b[39mget_func(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensordot\u001b[39m\u001b[38;5;124m'\u001b[39m, backend)\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/opt_einsum/backends/torch.py:54\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     51\u001b[0m torch, _ \u001b[38;5;241m=\u001b[39m _get_torch_and_device()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCH_HAS_TENSORDOT:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m xnd \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndimension()\n\u001b[1;32m     57\u001b[0m ynd \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mndimension()\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen/lib/python3.10/site-packages/torch/functional.py:1092\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims, out)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     dims_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(dims))\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_b\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mtensordot(a, b, dims_a, dims_b, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#from helpers.misc import print    #print function that displays time\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    for epoch in range(last_epoch+1, input_params.tot_epochs+1):\n",
    "\n",
    "        print(f'EPOCH {epoch}: Training...')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf':\n",
    "\n",
    "        #    meta = get_random_mask()\n",
    "\n",
    "        train_dataset.seq_df = train_df[train_df.train_fold == (epoch-1) % input_params.train_splits]\n",
    "        print(f'using train samples: {list(train_dataset.seq_df.index[[0,-1]])}')\n",
    "\n",
    "        train_metrics = train_eval.model_train(model, optimizer, train_dataloader, device,\n",
    "                            silent = False)\n",
    "        \n",
    "        print(f'epoch {epoch} - train, {metrics_to_str(train_metrics)}')\n",
    "\n",
    "        if epoch in input_params.save_at: #save model weights\n",
    "\n",
    "            misc.save_model_weights(model, optimizer, weights_dir, epoch)\n",
    "\n",
    "        if test_df is not None  and ( epoch==input_params.tot_epochs or\n",
    "                            (input_params.validate_every and epoch%input_params.validate_every==0)):\n",
    "\n",
    "            print(f'EPOCH {epoch}: Validating...')\n",
    "\n",
    "            val_metrics, *_ =  train_eval.model_eval(model, optimizer, test_dataloader, device,\n",
    "                    silent = False)\n",
    "\n",
    "            print(f'epoch {epoch} - validation, {metrics_to_str(val_metrics)}')\n",
    "            \n",
    "        #lr_scheduler.step()\n",
    "else:\n",
    "\n",
    "    print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "    test_metrics, test_embeddings, motif_probas =  train_eval.model_eval(model, optimizer, test_dataloader, device, \n",
    "                                                          get_embeddings = input_params.get_embeddings, \n",
    "                                                          silent = False)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "    if input_params.get_embeddings:\n",
    "        \n",
    "        os.makedirs(input_params.output_dir, exist_ok = True)\n",
    "\n",
    "        with open(input_params.output_dir + '/embeddings.pickle', 'wb') as f:\n",
    "            #test_embeddings = np.vstack(test_embeddings)\n",
    "            #np.save(f,test_embeddings)\n",
    "            pickle.dump(test_embeddings,f)\n",
    "            #pickle.dump(seq_df.seq_name.tolist(),f)\n",
    "            \n",
    "print()\n",
    "print(f'peak GPU memory allocation: {round(torch.cuda.max_memory_allocated(device)/1024/1024)} Mb')\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
