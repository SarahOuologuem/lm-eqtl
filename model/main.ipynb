{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae43e306-e5f1-4831-bb06-cecdb288fd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import pysam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be2f56b-e9fb-4abc-9339-a335bc412afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from encoding_utils import sequence_encoders\n",
    "import helpers.train_eval_a as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "from models.spec_dss import DSSResNetEmb, SpecAdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e119",
   "metadata": {},
   "source": [
    "# Train/Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79b474",
   "metadata": {},
   "source": [
    "**0. Specify input parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4edc775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.dataset =  '../extdata/datasets/phase3_top10/dataset.parquet'\n",
    "input_params.model_weight = '../extdata/checkpoints/phase3_top10/aware_large_splitmsk/weights/epoch_100_weights_model.pt'\n",
    "#input_params.species_list = datadir + 'fasta/240_species/240_species.txt'\n",
    "\n",
    "input_params.output_dir = '../results/test'\n",
    "\n",
    "input_params.split_mask = False\n",
    "input_params.mask_rate = 0.2 #[0.012,0.2]#RAN #single float or 2 floats for reference and alternative\n",
    "input_params.masking = 'none' # stratified_maf or none\n",
    "\n",
    "input_params.diploid = True\n",
    "\n",
    "input_params.test = True\n",
    "\n",
    "input_params.get_embeddings = False\n",
    "input_params.mask_at_test = True\n",
    "\n",
    "input_params.agnostic = False\n",
    "\n",
    "input_params.seq_len = 5000\n",
    "\n",
    "input_params.tot_epochs = 1\n",
    "input_params.fold = 0\n",
    "input_params.Nfolds = 5\n",
    "\n",
    "input_params.train_splits = 1\n",
    "\n",
    "input_params.save_at = [1]\n",
    "input_params.validate_every = 1\n",
    "\n",
    "input_params.d_model = 256\n",
    "input_params.n_layers = 16\n",
    "input_params.dropout = 0.\n",
    "\n",
    "input_params.batch_size = 16\n",
    "input_params.learning_rate = 1e-4\n",
    "input_params.weight_decay = 0\n",
    "\n",
    "if input_params.dataset.endswith('.fa'):\n",
    "    seq_df = pd.read_csv(input_params.dataset + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "elif input_params.dataset.endswith('.parquet'):\n",
    "    seq_df = pd.read_parquet(input_params.dataset).reset_index()\n",
    "    \n",
    "seq_df[['split','sample_id','seg_name']] =  seq_df['seq_name'].str.split(':',expand=True)\n",
    "\n",
    "if not input_params.agnostic:\n",
    "    #for segment-aware model, assign a label to each segment\n",
    "    seg_name = seq_df.seq_name.apply(lambda x:':'.join(x.split(':')[2:]))\n",
    "    segment_encoding = seg_name.drop_duplicates().reset_index(drop=True)\n",
    "    segment_encoding = {seg_name:idx for idx,seg_name in segment_encoding.items()}\n",
    "    seq_df['seg_label'] = seg_name.map(segment_encoding)\n",
    "else:\n",
    "    seq_df['seg_label'] = 0\n",
    "\n",
    "\n",
    "if input_params.test:\n",
    "    seq_df = seq_df[seq_df.split=='test']\n",
    "else:\n",
    "    seq_df = seq_df[seq_df.split!='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d126226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>seq</th>\n",
       "      <th>split</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>seg_name</th>\n",
       "      <th>seg_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20010</th>\n",
       "      <td>test:NA20795:ENSG00000198502.5</td>\n",
       "      <td>FBFFFFFRMRMMMFBFFMBRBRFFRFFFFFFFFMFFBFFFFFFFFB...</td>\n",
       "      <td>test</td>\n",
       "      <td>NA20795</td>\n",
       "      <td>ENSG00000198502.5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20011</th>\n",
       "      <td>test:HG00260:ENSG00000214425.1</td>\n",
       "      <td>RRBRRRBRRRBBRRBBBBRBBBRRBRBRRBRRRRBBBRBRRBBRRB...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00260</td>\n",
       "      <td>ENSG00000214425.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20012</th>\n",
       "      <td>test:HG01632:ENSG00000176681.9</td>\n",
       "      <td>BBRBBBRBBBBBBRBBBBRBBBBBBBBBBBBBBBBBRRBBBBRBBB...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG01632</td>\n",
       "      <td>ENSG00000176681.9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20013</th>\n",
       "      <td>test:HG00173:ENSG00000238083.3</td>\n",
       "      <td>RMRRRRRRRRRRRRRRRRFRRRRFRRRRRRMRRRRMRRRRRRRMRR...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00173</td>\n",
       "      <td>ENSG00000238083.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20014</th>\n",
       "      <td>test:HG00178:ENSG00000229450.2</td>\n",
       "      <td>RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRBBRRBRRRRRRRRRRR...</td>\n",
       "      <td>test</td>\n",
       "      <td>HG00178</td>\n",
       "      <td>ENSG00000229450.2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             seq_name  \\\n",
       "20010  test:NA20795:ENSG00000198502.5   \n",
       "20011  test:HG00260:ENSG00000214425.1   \n",
       "20012  test:HG01632:ENSG00000176681.9   \n",
       "20013  test:HG00173:ENSG00000238083.3   \n",
       "20014  test:HG00178:ENSG00000229450.2   \n",
       "\n",
       "                                                     seq split sample_id  \\\n",
       "20010  FBFFFFFRMRMMMFBFFMBRBRFFRFFFFFFFFMFFBFFFFFFFFB...  test   NA20795   \n",
       "20011  RRBRRRBRRRBBRRBBBBRBBBRRBRBRRBRRRRBBBRBRRBBRRB...  test   HG00260   \n",
       "20012  BBRBBBRBBBBBBRBBBBRBBBBBBBBBBBBBBBBBRRBBBBRBBB...  test   HG01632   \n",
       "20013  RMRRRRRRRRRRRRRRRRFRRRRFRRRRRRMRRRRMRRRRRRRMRR...  test   HG00173   \n",
       "20014  RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRBBRRBRRRRRRRRRRR...  test   HG00178   \n",
       "\n",
       "                seg_name  seg_label  \n",
       "20010  ENSG00000198502.5          4  \n",
       "20011  ENSG00000214425.1          1  \n",
       "20012  ENSG00000176681.9          9  \n",
       "20013  ENSG00000238083.3          0  \n",
       "20014  ENSG00000229450.2          3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82414099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "284231ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14ba6a",
   "metadata": {},
   "source": [
    "**1. Dataset and Dataloader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11c457",
   "metadata": {},
   "source": [
    "Define Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597bf5fb-c432-4eec-93da-17d6f10af4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seq_df, transform, max_augm_shift=0, \n",
    "                 mode='train'):\n",
    "\n",
    "        if input_params.dataset.endswith('.fa'):\n",
    "            self.fasta = pysam.FastaFile(input_params.dataset)\n",
    "        else:\n",
    "            self.fasta = None\n",
    "\n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "        self.max_augm_shift = max_augm_shift\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return 2*len(self.seq_df) # times two because returns both haplotypes \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.fasta:\n",
    "            seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        else:\n",
    "            seq = self.seq_df.iloc[idx].seq.upper()\n",
    "\n",
    "        shift = np.random.randint(self.max_augm_shift+1) #random shift at training, must be chunk_size-input_params.seq_len\n",
    "\n",
    "        seq = seq[shift:shift+input_params.seq_len] #shift the sequence and limit its size\n",
    "        seg_label = self.seq_df.iloc[idx].seg_label #label for segment-aware training\n",
    "        #'''\n",
    "        seq1 = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R') # father\n",
    "        seq2 = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R') # mother \n",
    "\n",
    "        masked_sequence1, target_labels_masked1, target_labels1, _, _ = self.transform(seq1)\n",
    "        masked_sequence2, target_labels_masked2, target_labels2, _, _ = self.transform(seq2)\n",
    "\n",
    "        masked_sequence = torch.vstack((masked_sequence1, masked_sequence2))\n",
    "        seg_label = torch.vstack((torch.tensor(seg_label), torch.tensor(seg_label)))\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "\n",
    "        target_labels_masked = torch.vstack((target_labels_masked1, target_labels_masked2))\n",
    "        target_labels = torch.vstack((target_labels1, target_labels2))\n",
    "        seq = (seq1, seq2)\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "        \n",
    "        '''\n",
    "        #for given genotype, randomly choose a haplotype for training/testing\n",
    "        if np.random.rand()>0.5:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R')\n",
    "        else:\n",
    "            seq = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf' and not input_params.test:\n",
    "        #    #select mask for the sequence depending on sequence coordinates w.r.t. contig\n",
    "        #    seg_name = self.seq_df.iloc[idx].seg_name\n",
    "        #    seq_mask = meta.loc[seg_name].MASK.values\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq, mask = seq_mask)\n",
    "        #else:\n",
    "        #    masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "\n",
    "        masked_sequence, target_labels_masked, target_labels, _, _ = self.transform(seq)\n",
    "\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "        '''\n",
    "\n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5510de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data): \n",
    "    #masked sequence\n",
    "    masked_sequence = [x[0][0] for x in data]\n",
    "    masked_sequence = [torch.stack(torch.split(d, 3)) for d in masked_sequence] \n",
    "    masked_sequence = torch.concat(masked_sequence)\n",
    "    #seg labels\n",
    "    seg_labels = [x[0][1] for x in data]\n",
    "    seg_labels = torch.concat(seg_labels).flatten()\n",
    "    # target labels masked\n",
    "    target_labels_masked = [x[1] for x in data]\n",
    "    target_labels_masked = torch.concat(target_labels_masked)\n",
    "    # target labels \n",
    "    target_labels = [x[2] for x in data]\n",
    "    target_labels = torch.concat(target_labels)\n",
    "    #seq\n",
    "    seqs = [x[3] for x in data]\n",
    "    seqs = tuple(chain.from_iterable(seqs))\n",
    "    return (masked_sequence, seg_labels),target_labels_masked, target_labels, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fa3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_get_embeddings(data): \n",
    "    # masked sequence\n",
    "    masked_sequence = [x[0][0] for x in data]\n",
    "    #print(masked_sequence[0].shape) \n",
    "    masked_sequence = [torch.stack(torch.split(d, split_size_or_sections=50, dim = 0)) for d in masked_sequence]\n",
    "    #print(masked_sequence[0].shape) \n",
    "    masked_sequence = torch.concat(masked_sequence)\n",
    "    #print(masked_sequence.shape)\n",
    "    # seg labels\n",
    "    seg_labels = [x[0][1] for x in data]\n",
    "    seg_labels = torch.concat(seg_labels).flatten()\n",
    "    # target labels masked\n",
    "    target_labels_masked = [x[1] for x in data]\n",
    "    target_labels_masked = [torch.stack(torch.split(d, split_size_or_sections=50, dim = 0)) for d in target_labels_masked] \n",
    "    target_labels_masked = torch.concat(target_labels_masked)\n",
    "    # target labels \n",
    "    target_labels = [x[2] for x in data]\n",
    "    target_labels = [torch.stack(torch.split(d, split_size_or_sections=50, dim = 0)) for d in target_labels]\n",
    "    target_labels = torch.concat(target_labels)\n",
    "    #seq\n",
    "    seqs = [x[3] for x in data]\n",
    "    seqs = tuple(chain.from_iterable(seqs))     \n",
    "    return (masked_sequence, seg_labels), target_labels_masked, target_labels, seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15637814",
   "metadata": {},
   "source": [
    "Create Dataset and Dataloader for the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ce1c8b-d70c-4fe7-ac01-e86fd30a8dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not getting embeddings\n"
     ]
    }
   ],
   "source": [
    "test_df = None \n",
    "\n",
    "if not input_params.test: #Train and Validate\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate = input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "\n",
    "    #N_train = int(len(seq_df)*(1-input_params.val_fraction))\n",
    "    if input_params.fold is not None:\n",
    "        \n",
    "        samples = seq_df.sample_id.unique()\n",
    "        val_samples = samples[input_params.fold::input_params.Nfolds] \n",
    "        train_df = seq_df[~seq_df.sample_id.isin(val_samples)] \n",
    "        test_df = seq_df[seq_df.sample_id.isin(val_samples)]\n",
    "        test_dataset = SeqDataset(test_df, transform = seq_transform, mode='eval')\n",
    "        test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 0, collate_fn = collate_fn, shuffle = False)\n",
    "    else:\n",
    "        train_df = seq_df\n",
    "        #train_df = seq_df[seq_df.split=='train']\n",
    "        #test_df = seq_df[seq_df.split=='val']\n",
    "  \n",
    "    N_train = len(train_df)\n",
    "    train_fold = np.repeat(list(range(input_params.train_splits)),repeats = N_train // input_params.train_splits + 1 )\n",
    "    train_df['train_fold'] = train_fold[:N_train]\n",
    "    # create training dataset & dataloader \n",
    "    train_dataset = SeqDataset(train_df, transform = seq_transform,  mode='train')\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_fn, shuffle = False)\n",
    "\n",
    "elif input_params.get_embeddings:\n",
    "    if input_params.mask_at_test:\n",
    "        seq_transform = sequence_encoders.RollingMasker(mask_stride = 50, frame = 0)\n",
    "    else:\n",
    "        seq_transform = sequence_encoders.PlainOneHot(frame = 0, padding = 'none')\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = collate_fn_get_embeddings, shuffle = False)\n",
    "\n",
    "else: #Test\n",
    "    print(\"not getting embeddings\")\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate=input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = SeqDataset(seq_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92832778",
   "metadata": {},
   "source": [
    "**2. Define model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad86179c-11e5-4b3b-a389-b8ab1b00bdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seg_encoder = SpecAdd(embed = True, encoder = 'label', Nsegments=seq_df.seg_label.nunique(), d_model = input_params.d_model)\n",
    "\n",
    "model = DSSResNetEmb(d_input = 3, d_output = 3, d_model = input_params.d_model, n_layers = input_params.n_layers, \n",
    "                     dropout = input_params.dropout, embed_before = True, species_encoder = seg_encoder)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = input_params.learning_rate, weight_decay = input_params.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806065f5",
   "metadata": {},
   "source": [
    "**3. Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec227158-c405-4dc7-8f6a-38a83009d725",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m last_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43minput_params\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_weight:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith loaded model weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_params' is not defined"
     ]
    }
   ],
   "source": [
    "last_epoch = 0\n",
    "\n",
    "if input_params.model_weight:\n",
    "    print(\"With loaded model weights:\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU...\")\n",
    "        #load on gpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight))\n",
    "    else:\n",
    "        #load on cpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight, map_location=torch.device('cpu')))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight, map_location=torch.device('cpu')))\n",
    "\n",
    "    last_epoch = int(input_params.model_weight.split('_')[-3]) #infer previous epoch from input_params.model_weight\n",
    "\n",
    "weights_dir = os.path.join(input_params.output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "\n",
    "if input_params.save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#        milestones=input_params.lr_sch_milestones, gamma=input_params.lr_sch_gamma, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f68efaa-d4a6-424f-a169-6d0d9046ec9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def metrics_to_str(metrics):\n",
    "    loss, accuracy, masked_acc, masked_recall, masked_IQS = metrics\n",
    "    return f'loss: {loss:.4}, acc: {accuracy:.4}, masked acc: {masked_acc:.4}, {misc.print_class_recall(masked_recall, \"masked recall: \")}, masked IQS: {masked_IQS:.4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd086013-fd62-4964-8ccc-8b9d928d727d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 100: Test/Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klumpi/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/conv.py:306: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 50.00 MiB is free. Including non-PyTorch memory, this process has 1.28 GiB memory in use. Of the allocated memory 340.28 MiB is allocated by PyTorch, and 33.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m#lr_scheduler.step()\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Test/Inference...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     test_metrics, test_embeddings, motif_probas \u001b[38;5;241m=\u001b[39m  \u001b[43mtrain_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiploid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiploid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - test, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_to_str(test_metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39mget_embeddings:\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/helpers/train_eval_a.py:284\u001b[0m, in \u001b[0;36mmodel_eval\u001b[0;34m(model, dataloader, device, diploid, get_embeddings, temperature, silent)\u001b[0m\n\u001b[1;32m    281\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diploid: \n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_eval_diploid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mget_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m MaskedAccuracy()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    288\u001b[0m masked_recall \u001b[38;5;241m=\u001b[39m MeanRecall()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/helpers/train_eval_a.py:214\u001b[0m, in \u001b[0;36mmodel_eval_diploid\u001b[0;34m(model, dataloader, device, get_embeddings, temperature, silent)\u001b[0m\n\u001b[1;32m    212\u001b[0m targets_masked \u001b[38;5;241m=\u001b[39m targets_masked\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    213\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 214\u001b[0m logits, embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature:\n\u001b[1;32m    216\u001b[0m     logits \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m temperature\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/models/spec_dss.py:191\u001b[0m, in \u001b[0;36mDSSResNetEmb.forward\u001b[0;34m(self, x, xs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecies_encoder(x,xs)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#print(\"shape after encoder\", x.size())\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, norm, dropout \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms4_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropouts):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     z \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/models/spec_dss.py:21\u001b[0m, in \u001b[0;36mL1Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 158.00 MiB. GPU 0 has a total capacty of 1.83 GiB of which 50.00 MiB is free. Including non-PyTorch memory, this process has 1.28 GiB memory in use. Of the allocated memory 340.28 MiB is allocated by PyTorch, and 33.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#from helpers.misc import print    #print function that displays time\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    for epoch in range(last_epoch+1, input_params.tot_epochs+1):\n",
    "\n",
    "        print(f'EPOCH {epoch}: Training...')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf':\n",
    "\n",
    "        #    meta = get_random_mask()\n",
    "\n",
    "        train_dataset.seq_df = train_df[train_df.train_fold == (epoch-1) % input_params.train_splits]\n",
    "        print(f'using train samples: {list(train_dataset.seq_df.index[[0,-1]])}')\n",
    "\n",
    "        train_metrics = train_eval.model_train(model, optimizer, train_dataloader, device,\n",
    "                            silent = False)\n",
    "        \n",
    "        print(f'epoch {epoch} - train, {metrics_to_str(train_metrics)}')\n",
    "\n",
    "        if epoch in input_params.save_at: #save model weights\n",
    "\n",
    "            misc.save_model_weights(model, optimizer, weights_dir, epoch)\n",
    "\n",
    "        if test_df is not None  and ( epoch==input_params.tot_epochs or\n",
    "                            (input_params.validate_every and epoch%input_params.validate_every==0)):\n",
    "\n",
    "            print(f'EPOCH {epoch}: Validating...')\n",
    "\n",
    "            val_metrics, *_ =  train_eval.model_eval(model, optimizer, test_dataloader, device,\n",
    "                    silent = False)\n",
    "\n",
    "            print(f'epoch {epoch} - validation, {metrics_to_str(val_metrics)}')\n",
    "            \n",
    "        #lr_scheduler.step()\n",
    "else:\n",
    "\n",
    "    print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "    test_metrics, test_embeddings, motif_probas =  train_eval.model_eval(model, test_dataloader, device, \n",
    "                                                          get_embeddings = input_params.get_embeddings, diploid=input_params.diploid,\n",
    "                                                          silent = False)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "    if input_params.get_embeddings:\n",
    "        \n",
    "        os.makedirs(input_params.output_dir, exist_ok = True)\n",
    "\n",
    "        with open(input_params.output_dir + '/embeddings.pickle', 'wb') as f:\n",
    "            #test_embeddings = np.vstack(test_embeddings)\n",
    "            #np.save(f,test_embeddings)\n",
    "            pickle.dump(test_embeddings,f)\n",
    "            #pickle.dump(seq_df.seq_name.tolist(),f)\n",
    "            \n",
    "print()\n",
    "print(f'peak GPU memory allocation: {round(torch.cuda.max_memory_allocated(device)/1024/1024)} Mb')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3f95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b2b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79584bc",
   "metadata": {},
   "source": [
    "### FOR CODE CHECKING, get_embeddings = FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3770e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_sequence, species_label), targets_masked, targets, seq = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aebe14a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/storage/miniconda3/envs/atac2space/lib/python3.11/site-packages/torch/nn/functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "from helpers.metrics import MeanRecall, MaskedAccuracy, IQS\n",
    "from helpers.misc import EMA, print_class_recall\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "temperature = None\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "accuracy = MaskedAccuracy().to(device)\n",
    "masked_recall = MeanRecall(Nclasses=4).to(device)\n",
    "masked_accuracy = MaskedAccuracy().to(device)\n",
    "masked_IQS = IQS(Nclasses=4).to(device)\n",
    "\n",
    "model.eval() #model to train mode\n",
    "avg_loss = 0.\n",
    "all_embeddings = []\n",
    "motif_probas = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, embeddings = model(masked_sequence, species_label)\n",
    "    if temperature:\n",
    "        logits /= temperature\n",
    "\n",
    "    loss = criterion(logits, targets_masked)\n",
    "    avg_loss += loss.item()\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    #  combine the predictions and compute the metrics \n",
    "        \n",
    "    # \"notation\":\n",
    "    # 0 = R\n",
    "    # 1 = M \n",
    "    # 2 = F\n",
    "    # 3 = B \n",
    "    # -100 = masked\n",
    "\n",
    "    targets = torch.split(targets, split_size_or_sections=2, dim=0) # each tensor contains predictions of both haplotypes  \n",
    "    targets_masked = torch.split(targets_masked, split_size_or_sections=2, dim=0)\n",
    "    preds = torch.split(preds, split_size_or_sections=2, dim=0)\n",
    "    combined_targets_all = []\n",
    "    combined_targets_masked_all = []\n",
    "    combined_preds_all = []\n",
    "    for (target, target_masked, pred) in zip(targets, targets_masked, preds):\n",
    "        #targets\n",
    "        combined_targets = target[0]+target[1]\n",
    "        combined_targets = torch.where(combined_targets==2, combined_targets +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_targets + target[0] # == 4 if both, 1 if father \n",
    "        combined_targets = torch.where(temp==1, temp+1, combined_targets) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_targets + target[1] # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_targets = torch.where(temp==1, temp, combined_targets) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "        combined_targets_all.append(combined_targets)\n",
    "        # combine masked targets \n",
    "        combined_targets_masked = target_masked[0]+target_masked[1]\n",
    "        combined_targets_masked = torch.where(combined_targets_masked==2, combined_targets_masked +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_targets_masked + target_masked[0] # == 4 if both, 1 if father \n",
    "        combined_targets_masked = torch.where(temp==1, temp+1, combined_targets_masked) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_targets_masked + target_masked[1] # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_targets_masked = torch.where(temp==1, temp, combined_targets_masked) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "        combined_targets_masked = torch.where(temp==-100, temp, combined_targets_masked)# == 3 if both, 2 if father, 1 if mother, -100 if masked, otherwise 0\n",
    "        combined_targets_masked_all.append(combined_targets_masked)\n",
    "        # combine preds \n",
    "        combined_preds = pred[0]+pred[1]\n",
    "        combined_preds = torch.where(combined_preds==2, combined_preds +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_preds + pred[0] # == 4 if both, 1 if father \n",
    "        combined_preds = torch.where(temp==1, temp+1, combined_preds) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_preds + pred[1] # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_preds = torch.where(temp==1, temp, combined_preds) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "        combined_preds_all.append(combined_preds)\n",
    "\n",
    "    accuracy.update(torch.stack(combined_preds_all), torch.stack(combined_targets_all))\n",
    "    masked_recall.update(torch.stack(combined_preds_all), torch.stack(combined_targets_masked_all))\n",
    "    masked_accuracy.update(torch.stack(combined_preds_all), torch.stack(combined_targets_masked_all))\n",
    "    masked_IQS.update(torch.stack(combined_preds_all), torch.stack(combined_targets_masked_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d146c970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9862),\n",
       " tensor(0.6469),\n",
       " array([0.80742776, 0.4119223 , 0.48987615, 0.9859594 ], dtype=float32),\n",
       " 0.5048352479934692)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(), masked_accuracy.compute(), masked_recall.compute(), masked_IQS.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d1fd087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5000])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cedc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5000])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "11eda1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 1,  ..., 0, 0, 1]]),\n",
       " tensor([[1, 1, 0,  ..., 1, 1, 1],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 1,  ..., 1, 0, 1],\n",
       "         [0, 0, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 1,  ..., 1, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 1, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[1, 0, 0,  ..., 1, 1, 1],\n",
       "         [1, 0, 0,  ..., 1, 1, 1]]),\n",
       " tensor([[0, 1, 0,  ..., 0, 1, 0],\n",
       "         [1, 1, 1,  ..., 0, 1, 0]]),\n",
       " tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(targets, split_size_or_sections=2, dim=0) # each tensor contains predictions of both haplotypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b77b64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([2, 3, 2,  ..., 2, 2, 2]),\n",
       " tensor([0, 0, 3,  ..., 0, 0, 1]),\n",
       " tensor([3, 3, 0,  ..., 2, 2, 2]),\n",
       " tensor([0, 1, 0,  ..., 2, 2, 2]),\n",
       " tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " tensor([3, 1, 1,  ..., 3, 3, 3]),\n",
       " tensor([0, 0, 2,  ..., 0, 0, 0]),\n",
       " tensor([0, 0, 3,  ..., 2, 0, 2]),\n",
       " tensor([0, 0, 2,  ..., 1, 1, 0]),\n",
       " tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " tensor([0, 0, 3,  ..., 1, 0, 0]),\n",
       " tensor([0, 1, 0,  ..., 0, 2, 0]),\n",
       " tensor([0, 3, 0,  ..., 0, 0, 0]),\n",
       " tensor([3, 0, 0,  ..., 3, 3, 3]),\n",
       " tensor([1, 3, 1,  ..., 0, 3, 0]),\n",
       " tensor([3, 3, 0,  ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_targets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1e472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7daa640a",
   "metadata": {},
   "source": [
    "### FOR CODE CHECKING, get_embeddings = TRUE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a955316",
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_sequence, species_label), targets_masked, targets, seq = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73bc81d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/storage/ouologuems/other/systems_genetics/lm-eqtl/model/models/dss.py:335: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:299.)\n",
      "  return einsum('chn,hnl->chl', W, S).float(), state                   # [C H L]\n",
      "/vol/storage/miniconda3/envs/atac2space/lib/python3.11/site-packages/torch/nn/functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    }
   ],
   "source": [
    "from helpers.metrics import MeanRecall, MaskedAccuracy, IQS\n",
    "from helpers.misc import EMA, print_class_recall\n",
    "from torch.nn.functional import log_softmax\n",
    "\n",
    "temperature = None\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "accuracy = MaskedAccuracy().to(device)\n",
    "masked_recall = MeanRecall(Nclasses=4).to(device)\n",
    "masked_accuracy = MaskedAccuracy().to(device)\n",
    "masked_IQS = IQS(Nclasses=4).to(device)\n",
    "\n",
    "model.eval() #model to train mode\n",
    "avg_loss = 0.\n",
    "all_embeddings = []\n",
    "motif_probas = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    if True:\n",
    "        # with batch size = 1, one batch contains the two sequences of one sample \n",
    "        # let's extract both sequences and get the predictions of both \n",
    "        # afterwards compute the scores for the combined predictions \n",
    "        #masked sequence\n",
    "        masked_sequence1 = torch.split(masked_sequence, split_size_or_sections = 1, dim = 0)[0][0]\n",
    "        masked_sequence2 = torch.split(masked_sequence, split_size_or_sections = 1, dim = 0)[1][0]\n",
    "        # targets_masked\n",
    "        targets_masked1 = torch.split(targets_masked, split_size_or_sections = 1, dim = 0)[0][0]\n",
    "        targets_masked2 = torch.split(targets_masked, split_size_or_sections = 1, dim = 0)[1][0]\n",
    "        # targets \n",
    "        targets1 = torch.split(targets, split_size_or_sections = 1, dim = 0)[0][0]\n",
    "        targets2 = torch.split(targets, split_size_or_sections = 1, dim = 0)[1][0]\n",
    "        species_label = species_label[0]\n",
    "        species_label = species_label.tile((len(masked_sequence1),))\n",
    "\n",
    "        masked_sequence1 = masked_sequence1.to(device)\n",
    "        masked_sequence2 = masked_sequence2.to(device)\n",
    "        targets_masked1 = targets_masked1.to(device)\n",
    "        targets_masked2 = targets_masked2.to(device)\n",
    "        targets1 = targets1.to(device) \n",
    "        targets2 = targets2.to(device)   \n",
    "        species_label = species_label.long().to(device)\n",
    "\n",
    "        logits1, embeddings1 = model(masked_sequence1, species_label)\n",
    "        if temperature:\n",
    "            logits1 /= temperature\n",
    "        loss1 = criterion(logits1, targets_masked1)\n",
    "        avg_loss += loss1.item()\n",
    "        preds1 = torch.argmax(logits1, dim=1)\n",
    "\n",
    "        logits2, embeddings2 = model(masked_sequence2, species_label)\n",
    "        if temperature:\n",
    "            logits2 /= temperature\n",
    "        loss2 = criterion(logits2, targets_masked1)\n",
    "        avg_loss += loss2.item()\n",
    "        preds2 = torch.argmax(logits2, dim=1)\n",
    "\n",
    "        # \"notation\":\n",
    "        # 0 = R\n",
    "        # 1 = M \n",
    "        # 2 = F\n",
    "        # 3 = B \n",
    "        # -100 = masked \n",
    "\n",
    "        # combine predictions\n",
    "        combined_preds = preds1+preds2\n",
    "        combined_preds = torch.where(combined_preds==2, combined_preds +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_preds + preds1 # == 4 if both, 1 if father \n",
    "        combined_preds = torch.where(temp==1, temp+1, combined_preds) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_preds + preds2 # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_preds = torch.where(temp==1, temp, combined_preds) # == 3 if both, 2 if father, 1 if mother, otherwise 0        \n",
    "                \n",
    "        # combine targets\n",
    "        combined_targets = targets1+targets2\n",
    "        combined_targets = torch.where(combined_targets==2, combined_targets +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_targets + targets1 # == 4 if both, 1 if father \n",
    "        combined_targets = torch.where(temp==1, temp+1, combined_targets) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_targets + targets2 # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_targets = torch.where(temp==1, temp, combined_targets) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "\n",
    "        # combine masked targets\n",
    "        combined_targets_masked = targets_masked1+targets_masked2\n",
    "        combined_targets_masked = torch.where(combined_targets_masked==2, combined_targets_masked +1, 0) # == 3 if both, 0 otherwise \n",
    "        temp = combined_targets_masked + targets_masked1 # == 4 if both, 1 if father \n",
    "        combined_targets_masked = torch.where(temp==1, temp+1, combined_targets_masked) # == 3 if both, 2 if father, otherwise 0 \n",
    "        temp = combined_targets_masked + targets_masked2 # == 4 if both, 2 if father, 1 if mother \n",
    "        combined_targets_masked = torch.where(temp==1, temp, combined_targets_masked) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "        combined_targets_masked = torch.where(temp==-100, temp, combined_targets_masked)# == 3 if both, 2 if father, 1 if mother, -100 if masked, otherwise 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RollingMasker: Creates a batch with all bases masked at some point in rolling mask fashion (50 different maskings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7e75fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 5000]), torch.Size([50, 5000]), torch.Size([50, 5000]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_masked1.shape, targets1.shape, preds1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05399470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b1b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58086bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9735),\n",
       " tensor(0.9576),\n",
       " array([0.97356606, 0.91608393, 0.92177314, 0.98029196], dtype=float32),\n",
       " 0.9402140974998474)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(), masked_accuracy.compute(), masked_recall.compute(), masked_IQS.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "996dac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "itr_idx = 0\n",
    "if  input_params.get_embeddings: \n",
    "\n",
    "    seq_name = test_dataloader.dataset.seq_df.iloc[itr_idx].seq_name # extract sequence ID \n",
    "\n",
    "    # get embeddings of the masked nucleotide\n",
    "    sequence_embedding1 = embeddings1[\"seq_embedding\"]\n",
    "    sequence_embedding1 = sequence_embedding1.transpose(-1,-2)[targets_masked1!=-100]\n",
    "    sequence_embedding2 = embeddings2[\"seq_embedding\"]\n",
    "    sequence_embedding2 = sequence_embedding2.transpose(-1,-2)[targets_masked2!=-100]\n",
    "    \n",
    "    sequence_embedding1 = sequence_embedding1.mean(dim=0) # if we mask\n",
    "    sequence_embedding2 = sequence_embedding2.mean(dim=0) # if we mask\n",
    "    sequence_embedding1 = sequence_embedding1.detach().cpu().numpy()\n",
    "    sequence_embedding2 = sequence_embedding2.detach().cpu().numpy()\n",
    "\n",
    "    logits1 = torch.permute(logits1,(2,0,1)).reshape(-1,masked_sequence1.shape[1]).detach()\n",
    "    targets_masked1 = targets_masked1.T.flatten()\n",
    "    masked_targets1 = targets_masked1[targets_masked1!=-100].cpu()\n",
    "    logits1 = logits1[targets_masked1!=-100].cpu()\n",
    "\n",
    "    logits2 = torch.permute(logits2,(2,0,1)).reshape(-1,masked_sequence2.shape[1]).detach()\n",
    "    targets_masked2 = targets_masked2.T.flatten()\n",
    "    masked_targets2 = targets_masked2[targets_masked2!=-100].cpu()\n",
    "    logits2 = logits2[targets_masked2!=-100].cpu()    \n",
    "\n",
    "    logprobs1 = log_softmax(logits1, dim=1).numpy()\n",
    "    ground_truth_logprobs1 = np.array([logprobs1[idx,base] for idx,base in enumerate(masked_targets1)])\n",
    "    all_embeddings.append((seq_name + \":father\",sequence_embedding1,ground_truth_logprobs1))\n",
    "\n",
    "    logprobs2 = log_softmax(logits2, dim=1).numpy()\n",
    "    ground_truth_logprobs2 = np.array([logprobs2[idx,base] for idx,base in enumerate(masked_targets2)])\n",
    "    all_embeddings.append((seq_name + \":mother\",sequence_embedding2,ground_truth_logprobs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6efe63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8d439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = R\n",
    "# 1 = M \n",
    "# 2 = F\n",
    "# 3 = B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2bd4e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2,  ..., 2, 0, 0],\n",
       "        [2, 2, 2,  ..., 2, 0, 0],\n",
       "        [2, 2, 2,  ..., 2, 0, 0],\n",
       "        ...,\n",
       "        [2, 2, 2,  ..., 2, 0, 0],\n",
       "        [2, 2, 2,  ..., 1, 0, 0],\n",
       "        [2, 2, 2,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine predictions\n",
    "combined_preds = preds1+preds2\n",
    "combined_preds = torch.where(combined_preds==2, combined_preds +1, 0) # == 3 if both, 0 otherwise \n",
    "temp = combined_preds + preds1 # == 4 if both, 1 if father \n",
    "combined_preds = torch.where(temp==1, temp+1, combined_preds) # == 3 if both, 2 if father, otherwise 0 \n",
    "temp = combined_preds + preds2 # == 4 if both, 2 if father, 1 if mother \n",
    "combined_preds = torch.where(temp==1, temp, combined_preds) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "combined_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1481fb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 2,  ..., 2, 2, 2],\n",
       "        [2, 3, 2,  ..., 2, 2, 2],\n",
       "        [2, 3, 2,  ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [2, 3, 2,  ..., 2, 2, 2],\n",
       "        [2, 3, 2,  ..., 2, 2, 2],\n",
       "        [2, 3, 2,  ..., 2, 2, 2]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine targets\n",
    "combined_targets = targets1+targets2\n",
    "combined_targets = torch.where(combined_targets==2, combined_targets +1, 0) # == 3 if both, 0 otherwise \n",
    "temp = combined_targets + targets1 # == 4 if both, 1 if father \n",
    "combined_targets = torch.where(temp==1, temp+1, combined_targets) # == 3 if both, 2 if father, otherwise 0 \n",
    "temp = combined_targets + targets2 # == 4 if both, 2 if father, 1 if mother \n",
    "combined_targets = torch.where(temp==1, temp, combined_targets) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "combined_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dbddc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100,    3, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100,    2,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ...,    2, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100,    2, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100,    2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine masked targets\n",
    "combined_targets_masked = targets_masked1+targets_masked2\n",
    "combined_targets_masked = torch.where(combined_targets_masked==2, combined_targets_masked +1, 0) # == 3 if both, 0 otherwise \n",
    "temp = combined_targets_masked + targets_masked1 # == 4 if both, 1 if father \n",
    "combined_targets_masked = torch.where(temp==1, temp+1, combined_targets_masked) # == 3 if both, 2 if father, otherwise 0 \n",
    "temp = combined_targets_masked + targets_masked2 # == 4 if both, 2 if father, 1 if mother \n",
    "combined_targets_masked = torch.where(temp==1, temp, combined_targets_masked) # == 3 if both, 2 if father, 1 if mother, otherwise 0 \n",
    "combined_targets_masked = torch.where(temp==-100, temp, combined_targets_masked)# == 3 if both, 2 if father, 1 if mother, -100 if masked, otherwise 0 \n",
    "combined_targets_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70608ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100,    1, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100,    1,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ...,    1, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100,    1, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100,    1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_masked1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "857a156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100,    1, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100,    0,  ..., -100, -100, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ...,    0, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100,    0, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100,    0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_masked2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
