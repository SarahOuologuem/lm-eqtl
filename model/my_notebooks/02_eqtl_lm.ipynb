{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict expression from genotype\n",
    "\n",
    "1. Load weights from trained LM and freeze\n",
    "2. Take only the encoder part\n",
    "3. Add a prediction head -> learnable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import pysam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from itertools import chain\n",
    "from torch import nn\n",
    "\n",
    "if not \"../\" in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from encoding_utils import sequence_encoders\n",
    "import helpers.train_eval_a as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "from models.spec_dss import DSSResNetEmb, SpecAdd, DSSResNetExpression\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the original model with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_params = misc.dotdict({})\n",
    "\n",
    "input_params.dataset =  '../../extdata/datasets/phase3_top10/dataset.parquet'\n",
    "input_params.model_weight = '../../extdata/checkpoints/phase3_top10/aware_large_splitmsk/weights/epoch_100_weights_model.pt'\n",
    "\n",
    "\n",
    "input_params.output_dir = '../../results/test'\n",
    "\n",
    "input_params.split_mask = False\n",
    "input_params.mask_rate = 0.2 #[0.012,0.2]#RAN #single float or 2 floats for reference and alternative\n",
    "input_params.masking = 'none' # stratified_maf or none\n",
    "\n",
    "input_params.diploid = True\n",
    "\n",
    "input_params.test = False\n",
    "\n",
    "input_params.get_embeddings = False\n",
    "input_params.mask_at_test = True\n",
    "\n",
    "input_params.agnostic = False\n",
    "\n",
    "input_params.seq_len = 500\n",
    "\n",
    "input_params.tot_epochs = 1\n",
    "input_params.fold = 0\n",
    "input_params.Nfolds = 5\n",
    "\n",
    "input_params.train_splits = 1\n",
    "\n",
    "input_params.save_at = [1]\n",
    "input_params.validate_every = 1\n",
    "\n",
    "input_params.d_model = 256\n",
    "input_params.n_layers = 8\n",
    "input_params.dropout = 0.\n",
    "\n",
    "input_params.batch_size = 8\n",
    "input_params.learning_rate = 1e-4\n",
    "input_params.weight_decay = 0\n",
    "\n",
    "if input_params.dataset.endswith('.fa'):\n",
    "    seq_df = pd.read_csv(input_params.dataset + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "elif input_params.dataset.endswith('.parquet'):\n",
    "    seq_df = pd.read_parquet(input_params.dataset).reset_index()\n",
    "    \n",
    "seq_df[['split','sample_id','seg_name']] =  seq_df['seq_name'].str.split(':',expand=True)\n",
    "\n",
    "if not input_params.agnostic:\n",
    "    #for segment-aware model, assign a label to each segment\n",
    "    seg_name = seq_df.seq_name.apply(lambda x:':'.join(x.split(':')[2:]))\n",
    "    segment_encoding = seg_name.drop_duplicates().reset_index(drop=True)\n",
    "    segment_encoding = {seg_name:idx for idx,seg_name in segment_encoding.items()}\n",
    "    seq_df['seg_label'] = seg_name.map(segment_encoding)\n",
    "else:\n",
    "    seq_df['seg_label'] = 0\n",
    "\n",
    "\n",
    "if input_params.test:\n",
    "    seq_df = seq_df[seq_df.split=='test']\n",
    "else:\n",
    "    seq_df = seq_df[seq_df.split!='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_data = pd.read_csv(\"../../extdata/GD660.GeneQuantRPKM.txt.gz\", sep=\"\\t\")\\\n",
    "    .rename(columns=lambda x: re.sub(\"\\..*\",'',x))\\\n",
    "    .melt(id_vars = [\"TargetID\", \"Gene_Symbol\", \"Chr\", \"Coord\"], var_name = \"sample_id\", value_name = \"expr\")[[\"Gene_Symbol\", \"sample_id\", \"expr\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_expression_df = pd.merge(seq_df, expression_data, left_on=[\"sample_id\", \"seg_name\"], right_on=[\"sample_id\", \"Gene_Symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = \"cpu\"\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seq_df, transform, max_augm_shift=0, \n",
    "                 mode='train'):\n",
    "\n",
    "        if input_params.dataset.endswith('.fa'):\n",
    "            self.fasta = pysam.FastaFile(input_params.dataset)\n",
    "        else:\n",
    "            self.fasta = None\n",
    "\n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "        self.max_augm_shift = max_augm_shift\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return 2*len(self.seq_df) # times two because returns both haplotypes \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.fasta:\n",
    "            seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        else:\n",
    "            seq = self.seq_df.iloc[idx].seq.upper()\n",
    "\n",
    "        shift = np.random.randint(self.max_augm_shift+1) #random shift at training, must be chunk_size-input_params.seq_len\n",
    "\n",
    "        seq = seq[shift:shift+input_params.seq_len] #shift the sequence and limit its size\n",
    "        seg_label = self.seq_df.iloc[idx].seg_label #label for segment-aware training\n",
    "        #'''\n",
    "        seq1 = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R') # father\n",
    "        seq2 = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R') # mother \n",
    "\n",
    "        masked_sequence1, target_labels_masked1, target_labels1, _, _ = self.transform(seq1)\n",
    "        masked_sequence2, target_labels_masked2, target_labels2, _, _ = self.transform(seq2)\n",
    "\n",
    "        masked_sequence = torch.vstack((masked_sequence1, masked_sequence2))\n",
    "        seg_label = torch.vstack((torch.tensor(seg_label), torch.tensor(seg_label)))\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "\n",
    "        target_labels_masked = torch.vstack((target_labels_masked1, target_labels_masked2))\n",
    "        target_labels = torch.vstack((target_labels1, target_labels2))\n",
    "        seq = (seq1, seq2)\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq\n",
    "        \n",
    "\n",
    "    def close(self):\n",
    "        self.fasta.close()\n",
    "\n",
    "\n",
    "class ExpressionDataset(SeqDataset): \n",
    "\n",
    "    def __init__(self, *args, **kwargs): \n",
    "        super().__init__(*args, **kwargs)\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if self.fasta:\n",
    "            seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        else:\n",
    "            seq = self.seq_df.iloc[idx].seq.upper()\n",
    "\n",
    "        shift = np.random.randint(self.max_augm_shift+1) #random shift at training, must be chunk_size-input_params.seq_len\n",
    "\n",
    "        seq = seq[shift:shift+input_params.seq_len] #shift the sequence and limit its size\n",
    "        seg_label = self.seq_df.iloc[idx].seg_label #label for segment-aware training\n",
    "        #'''\n",
    "        seq1 = seq.replace('-','').replace('B','A').replace('F','A').replace('M','R') # father\n",
    "        seq2 = seq.replace('-','').replace('B','A').replace('M','A').replace('F','R') # mother \n",
    "\n",
    "        masked_sequence1, target_labels_masked1, target_labels1, _, _ = self.transform(seq1)\n",
    "        masked_sequence2, target_labels_masked2, target_labels2, _, _ = self.transform(seq2)\n",
    "\n",
    "        masked_sequence = torch.vstack((masked_sequence1, masked_sequence2))\n",
    "        seg_label = torch.vstack((torch.tensor(seg_label), torch.tensor(seg_label)))\n",
    "        masked_sequence = (masked_sequence, seg_label)\n",
    "\n",
    "        target_labels_masked = torch.vstack((target_labels_masked1, target_labels_masked2))\n",
    "        target_labels = torch.vstack((target_labels1, target_labels2))\n",
    "        seq = (seq1, seq2)\n",
    "\n",
    "        seq_expr = self.seq_df.iloc[idx].expr.astype(np.float32)\n",
    "\n",
    "        return masked_sequence, target_labels_masked, target_labels, seq, seq_expr\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def collate_expression_fn(data):\n",
    "    \"\"\"collate fn that adds expression values for each sequence.\n",
    "    \"\"\" \n",
    "    #masked sequence\n",
    "    masked_sequence = [x[0][0] for x in data]\n",
    "    masked_sequence = [torch.stack(torch.split(d, 3)) for d in masked_sequence] \n",
    "    masked_sequence = torch.concat(masked_sequence)\n",
    "    #seg labels\n",
    "    seg_labels = [x[0][1] for x in data]\n",
    "    seg_labels = torch.concat(seg_labels).flatten()\n",
    "    # target labels masked\n",
    "    target_labels_masked = [x[1] for x in data]\n",
    "    target_labels_masked = torch.concat(target_labels_masked)\n",
    "    # target labels \n",
    "    target_labels = [x[2] for x in data]\n",
    "    target_labels = torch.concat(target_labels)\n",
    "    #seq\n",
    "    seqs = [x[3] for x in data]\n",
    "    seqs = tuple(chain.from_iterable(seqs))\n",
    "\n",
    "    seg_expr = [x[4] for x in data]\n",
    "    # repeat each element twice, once for each haplotype\n",
    "    seg_expr = torch.Tensor(seg_expr).repeat_interleave(2)\n",
    "\n",
    "    return (masked_sequence, seg_labels, seg_expr),target_labels_masked, target_labels, seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27370/680069974.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['train_fold'] = train_fold[:N_train]\n"
     ]
    }
   ],
   "source": [
    "test_df = None \n",
    "\n",
    "if not input_params.test: #Train and Validate\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate = input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "\n",
    "    #N_train = int(len(seq_expression_df)*(1-input_params.val_fraction))\n",
    "    if input_params.fold is not None:\n",
    "        \n",
    "        samples = seq_expression_df.sample_id.unique()\n",
    "        val_samples = samples[input_params.fold::input_params.Nfolds] \n",
    "        train_df = seq_expression_df[~seq_expression_df.sample_id.isin(val_samples)] \n",
    "        test_df = seq_expression_df[seq_expression_df.sample_id.isin(val_samples)]\n",
    "        test_dataset = ExpressionDataset(test_df, transform = seq_transform, mode='eval')\n",
    "        test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 0, collate_fn = collate_expression_fn, shuffle = False)\n",
    "    else:\n",
    "        train_df = seq_expression_df\n",
    "        #train_df = seq_expression_df[seq_expression_df.split=='train']\n",
    "        #test_df = seq_expression_df[seq_expression_df.split=='val']\n",
    "  \n",
    "    N_train = len(train_df)\n",
    "    train_fold = np.repeat(list(range(input_params.train_splits)),repeats = N_train // input_params.train_splits + 1 )\n",
    "    train_df['train_fold'] = train_fold[:N_train]\n",
    "    # create training dataset & dataloader \n",
    "    train_dataset = ExpressionDataset(train_df, transform = seq_transform,  mode='train')\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_expression_fn, shuffle = False)\n",
    "\n",
    "elif input_params.get_embeddings:\n",
    "    if input_params.mask_at_test:\n",
    "        seq_transform = sequence_encoders.RollingMasker(mask_stride = 50, frame = 0)\n",
    "    else:\n",
    "        seq_transform = sequence_encoders.PlainOneHot(frame = 0, padding = 'none')\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = ExpressionDataset(seq_expression_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = collate_expression_fn, shuffle = False)\n",
    "\n",
    "else: #Test\n",
    "    print(\"not getting embeddings\")\n",
    "    seq_transform = sequence_encoders.SequenceDataEncoder(seq_len = input_params.seq_len, total_len = input_params.seq_len,\n",
    "                                                      mask_rate=input_params.mask_rate, split_mask = input_params.split_mask)\n",
    "    # create test dataset & dataloader \n",
    "    test_dataset = ExpressionDataset(seq_expression_df, transform = seq_transform, mode='eval')\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = input_params.batch_size, num_workers = 2, collate_fn = collate_expression_fn, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_encoder = SpecAdd(embed = True, encoder = 'label', Nsegments=seq_df.seg_label.nunique(), d_model = input_params.d_model)\n",
    "\n",
    "model = DSSResNetExpression(d_input = 3, d_output = 3, d_model = input_params.d_model, n_layers = input_params.n_layers, \n",
    "                     dropout = input_params.dropout, embed_before = True, species_encoder = seg_encoder)\n",
    "\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# load the weights\n",
    "model.load_state_dict(torch.load(input_params.model_weight, map_location=device), strict=False)\n",
    "\n",
    "# # get the encoder part of the model\n",
    "# encoder = DSSResNetEncoder(full_model)\n",
    "\n",
    "# model = DSSResNetExpression(encoder = encoder, d_encoder = 256, d_output = 1, freeze_encoder = True)\n",
    "\n",
    "# define which layers to freeze\n",
    "for param, weights in model.state_dict().items(): \n",
    "    if not param.startswith(\"regression\"):\n",
    "        weights.requires_grad = False\n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = input_params.learning_rate, weight_decay = input_params.weight_decay)\n",
    "\n",
    "weights_dir = os.path.join(input_params.output_dir, 'weights') #dir to save model weights at save_at epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_str(metrics):\n",
    "    expr_loss, loss, accuracy, masked_acc, masked_recall, masked_IQS = metrics\n",
    "    return f'expr_loss: {expr_loss:.4}, loss: {loss:.4}, acc: {accuracy:.4}, masked acc: {masked_acc:.4}, {misc.print_class_recall(masked_recall, \"masked recall: \")}, masked IQS: {masked_IQS:.4}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.metrics import MeanRecall, MaskedAccuracy, IQS\n",
    "from helpers.misc import EMA, print_class_recall\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_reg_model(model, optimizer, dataloader, device, silent=False):\n",
    "    \"\"\"train function with added regression loss\"\"\"\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "    accuracy = MaskedAccuracy(smooth=True).to(device)\n",
    "    masked_recall = MeanRecall().to(device)\n",
    "    masked_accuracy = MaskedAccuracy(smooth=True).to(device)\n",
    "    masked_IQS = IQS().to(device)\n",
    "\n",
    "    reg_criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train() #model to train mode\n",
    "\n",
    "    if not silent:\n",
    "        tot_itr = len(dataloader.dataset)//dataloader.batch_size #total train iterations\n",
    "        pbar = tqdm(total = tot_itr, ncols=750) #progress bar\n",
    "\n",
    "    loss_EMA = EMA()\n",
    "\n",
    "    for itr_idx, ((masked_sequence, species_label, expr), targets_masked, targets, _) in enumerate(dataloader):\n",
    "\n",
    "        masked_sequence = masked_sequence.to(device)\n",
    "        species_label = species_label.to(device)\n",
    "        targets_masked = targets_masked.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, _, expr_pred = model(masked_sequence, species_label)\n",
    "\n",
    "        masked_loss = criterion(logits, targets_masked)\n",
    "        expr_loss = reg_criterion(expr_pred, expr)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = expr_loss + masked_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #if max_abs_grad:\n",
    "        #    torch.nn.utils.clip_grad_value_(model.parameters(), max_abs_grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        smoothed_loss = loss_EMA.update(loss.item())\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        accuracy.update(preds, targets)\n",
    "        masked_recall.update(preds, targets_masked)\n",
    "        masked_accuracy.update(preds, targets_masked)\n",
    "        masked_IQS.update(preds, targets_masked)\n",
    "        \n",
    "        if not silent:\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f'loss: {expr_loss:.4}, acc: {accuracy.compute():.4}, {print_class_recall(masked_recall.compute(), \"masked recall: \")}, masked acc: {masked_accuracy.compute():.4}, masked IQS: {masked_IQS.compute():.4}, loss: {smoothed_loss:.4}')\n",
    "\n",
    "    if not silent:\n",
    "        pbar.reset()\n",
    "        del pbar\n",
    "\n",
    "    return expr_loss, loss, accuracy, masked_accuracy, masked_recall, masked_IQS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1: Training...\n",
      "using train samples: [2, 1549]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/310 [00:00<?, ?it/s]/home/klumpi/Uni/courses/SysGen/lm-eqtl/model/my_notebooks/../models/dss.py:335: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/Copy.cpp:299.)\n",
      "  return einsum('chn,hnl->chl', W, S).float(), state                   # [C H L]\n",
      "/home/klumpi/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/functional.py:1352: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
      "/home/klumpi/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "loss: nan, acc: 0.6287, masked recall: R=0.749;A=0.263;AVG=0.506, masked acc: 0.6416, masked IQS: 0.09914, loss: nan:   8%|█████████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 24/310 [01:34<18:19,  3.85s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mseq_df \u001b[38;5;241m=\u001b[39m train_df[train_df\u001b[38;5;241m.\u001b[39mtrain_fold \u001b[38;5;241m==\u001b[39m (epoch\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m input_params\u001b[38;5;241m.\u001b[39mtrain_splits]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musing train samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mseq_df\u001b[38;5;241m.\u001b[39mindex[[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_reg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_to_str(train_metrics)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39msave_at: \u001b[38;5;66;03m#save model weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mtrain_reg_model\u001b[0;34m(model, optimizer, dataloader, device, silent)\u001b[0m\n\u001b[1;32m     29\u001b[0m targets_masked \u001b[38;5;241m=\u001b[39m targets_masked\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 32\u001b[0m logits, _, expr_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m masked_loss \u001b[38;5;241m=\u001b[39m criterion(logits, targets_masked)\n\u001b[1;32m     35\u001b[0m expr_loss \u001b[38;5;241m=\u001b[39m reg_criterion(expr_pred, expr)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/my_notebooks/../models/spec_dss.py:426\u001b[0m, in \u001b[0;36mDSSResNetExpression.forward\u001b[0;34m(self, x, xs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     z \u001b[38;5;241m=\u001b[39m norm(z\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Apply S4 block: we ignore the state input and output\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m z, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Dropout on the output of the S4 block\u001b[39;00m\n\u001b[1;32m    429\u001b[0m z \u001b[38;5;241m=\u001b[39m dropout(z)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/my_notebooks/../models/dss.py:432\u001b[0m, in \u001b[0;36mDSS.forward\u001b[0;34m(self, u, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m#print(u.shape)   \u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Compute SS Kernel\u001b[39;00m\n\u001b[1;32m    431\u001b[0m Lk \u001b[38;5;241m=\u001b[39m L \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_kernel_length \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_kernel_length, L)\n\u001b[0;32m--> 432\u001b[0m k, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (C H Lk) (B C H Lk)\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#print(k.shape)\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# Convolution\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional:\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Uni/courses/SysGen/lm-eqtl/model/my_notebooks/../models/dss.py:335\u001b[0m, in \u001b[0;36mDSSKernel.forward\u001b[0;34m(self, L, state)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno-scale\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion:\n\u001b[1;32m    333\u001b[0m         W \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m*\u001b[39m (dt_Lambda\u001b[38;5;241m.\u001b[39mexp() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m reciprocal(Lambda, clamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# [C H N]\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchn,hnl->chl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat(), state\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/opt_einsum/contract.py:507\u001b[0m, in \u001b[0;36mcontract\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gen_expression:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ContractExpression(full_str, contraction_list, constants_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meinsum_kwargs)\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontraction_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/opt_einsum/contract.py:591\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         einsum_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m out_array\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# Do the contraction\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     new_view \u001b[38;5;241m=\u001b[39m \u001b[43m_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Append new items and dereference what we can\u001b[39;00m\n\u001b[1;32m    594\u001b[0m operands\u001b[38;5;241m.\u001b[39mappend(new_view)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/opt_einsum/sharing.py:151\u001b[0m, in \u001b[0;36meinsum_cache_wrap.<locals>.cached_einsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(einsum)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_einsum\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# hash modulo commutativity by computing a canonical ordering and names\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     backend \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/opt_einsum/contract.py:353\u001b[0m, in \u001b[0;36m_einsum\u001b[0;34m(*operands, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         einsum_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m parser\u001b[38;5;241m.\u001b[39mfind_output_str(einsum_str)\n\u001b[1;32m    351\u001b[0m     einsum_str \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mconvert_to_valid_einsum_chars(einsum_str)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/opt_einsum/backends/torch.py:45\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m     42\u001b[0m equation \u001b[38;5;241m=\u001b[39m convert_to_valid_einsum_chars(equation)\n\u001b[1;32m     44\u001b[0m torch, _ \u001b[38;5;241m=\u001b[39m _get_torch_and_device()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/functional.py:372\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    369\u001b[0m     _operands \u001b[38;5;241m=\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_operands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helpers import misc\n",
    "\n",
    "from IPython.display import clear_output\n",
    "last_epoch = 0\n",
    "\n",
    "clear_output()\n",
    "\n",
    "#from helpers.misc import print    #print function that displays time\n",
    "\n",
    "if not input_params.test:\n",
    "\n",
    "    for epoch in range(last_epoch+1, input_params.tot_epochs+1):\n",
    "\n",
    "        print(f'EPOCH {epoch}: Training...')\n",
    "\n",
    "        #if input_params.masking == 'stratified_maf':\n",
    "\n",
    "        #    meta = get_random_mask()\n",
    "\n",
    "        train_dataset.seq_df = train_df[train_df.train_fold == (epoch-1) % input_params.train_splits]\n",
    "        print(f'using train samples: {list(train_dataset.seq_df.index[[0,-1]])}')\n",
    "\n",
    "        train_metrics = train_reg_model(model, optimizer, train_dataloader, device,\n",
    "                            silent = False)\n",
    "            \n",
    "        print(f'epoch {epoch} - train, {metrics_to_str(train_metrics)}')\n",
    "\n",
    "        if epoch in input_params.save_at: #save model weights\n",
    "\n",
    "            misc.save_model_weights(model, optimizer, weights_dir, epoch)\n",
    "\n",
    "        if test_df is not None  and ( epoch==input_params.tot_epochs or\n",
    "                            (input_params.validate_every and epoch%input_params.validate_every==0)):\n",
    "\n",
    "            print(f'EPOCH {epoch}: Validating...')\n",
    "\n",
    "            val_metrics, *_ =  train_eval.model_eval(model, optimizer, test_dataloader, device,\n",
    "                    silent = False)\n",
    "\n",
    "            print(f'epoch {epoch} - validation, {metrics_to_str(val_metrics)}')\n",
    "            \n",
    "        #lr_scheduler.step()\n",
    "else:\n",
    "\n",
    "    print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "    test_metrics, test_embeddings, motif_probas =  train_eval.model_eval(model, test_dataloader, device, \n",
    "                                                          get_embeddings = input_params.get_embeddings, diploid=input_params.diploid,\n",
    "                                                          silent = False)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "    if input_params.get_embeddings:\n",
    "        \n",
    "        os.makedirs(input_params.output_dir, exist_ok = True)\n",
    "\n",
    "        with open(input_params.output_dir + '/embeddings.pickle', 'wb') as f:\n",
    "            #test_embeddings = np.vstack(test_embeddings)\n",
    "            #np.save(f,test_embeddings)\n",
    "            pickle.dump(test_embeddings,f)\n",
    "            #pickle.dump(seq_df.seq_name.tolist(),f)\n",
    "            \n",
    "print()\n",
    "print(f'peak GPU memory allocation: {round(torch.cuda.max_memory_allocated(device)/1024/1024)} Mb')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With loaded model weights:\n",
      "Using GPU...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DSSResNetExpression:\n\tMissing key(s) in state_dict: \"regression_head.0.weight\", \"regression_head.0.bias\", \"regression_head.2.weight\", \"regression_head.2.bias\", \"regression_head.4.weight\", \"regression_head.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#load on gpu\u001b[39;00m\n",
      "\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_params\u001b[38;5;241m.\u001b[39moptimizer_weight:\n",
      "\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(input_params\u001b[38;5;241m.\u001b[39moptimizer_weight))\n",
      "\n",
      "File \u001b[0;32m~/miniconda3/envs/sysgen_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n",
      "\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n",
      "\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n",
      "\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n",
      "\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n",
      "\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DSSResNetExpression:\n",
      "\tMissing key(s) in state_dict: \"regression_head.0.weight\", \"regression_head.0.bias\", \"regression_head.2.weight\", \"regression_head.2.bias\", \"regression_head.4.weight\", \"regression_head.4.bias\". "
     ]
    }
   ],
   "source": [
    "last_epoch = 0\n",
    "\n",
    "if input_params.model_weight:\n",
    "    print(\"With loaded model weights:\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using GPU...\")\n",
    "        #load on gpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight))\n",
    "    else:\n",
    "        #load on cpu\n",
    "        model.load_state_dict(torch.load(input_params.model_weight, map_location=torch.device('cpu')))\n",
    "        if input_params.optimizer_weight:\n",
    "            optimizer.load_state_dict(torch.load(input_params.optimizer_weight, map_location=torch.device('cpu')))\n",
    "\n",
    "    last_epoch = int(input_params.model_weight.split('_')[-3]) #infer previous epoch from input_params.model_weight\n",
    "\n",
    "weights_dir = os.path.join(input_params.output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "\n",
    "if input_params.save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "#        milestones=input_params.lr_sch_milestones, gamma=input_params.lr_sch_gamma, verbose=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sysgen_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
